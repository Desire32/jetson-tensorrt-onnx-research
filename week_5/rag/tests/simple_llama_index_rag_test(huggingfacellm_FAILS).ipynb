{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install llama_index llama_index.llms.huggingface llama_index.embeddings.huggingface"
      ],
      "metadata": {
        "id": "qPbVI-81QjCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "import time\n",
        "\n",
        "llm_models = [\"HuggingFaceTB/SmolLM2-135M\", \"HuggingFaceTB/SmolLM2-360M-Instruct\"]\n",
        "embed_models = [\"mixedbread-ai/mxbai-embed-large-v1\", \"google/embeddinggemma-300m\"]\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(embed_models[0])\n",
        "\n",
        "\"\"\"\n",
        "GRANITE INTEGRATIONS WORK VERY BADLY, CUDA OUT OF MEMORY\n",
        "\"\"\"\n",
        "llm = HuggingFaceLLM(\n",
        "    model_name=llm_models[0],\n",
        "    tokenizer_name=llm_models[0],\n",
        "    max_new_tokens=512,\n",
        "    model_kwargs={\n",
        "        # \"dtype\": \"bfloat16\", # might be changed [bfloat16, float16, float32, auto]\n",
        "        # \"load_in_4bit\": True # even less memory [load_in_4bit, load_in_8bit]\n",
        "        },\n",
        "    device_map=\"cuda\"\n",
        ")\n",
        "\n",
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
      ],
      "metadata": {
        "id": "nY1P_pA2tWyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine(llm=llm)\n",
        "\n",
        "response = query_engine.query(\"What can you tell me about Angeloktistisi church?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "tEqT47fwz_0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NbDb89K01qDO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}