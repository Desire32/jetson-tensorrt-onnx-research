{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install llama_index llama_index.llms.ollama llama_index.embeddings.huggingface"
      ],
      "metadata": {
        "id": "zZ2MJuMxF0FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "[https://colab.research.google.com/github/5aharsh/collama/blob/main/Ollama_Setup.ipynb#scrollTo=Jh5CBAFxBYAC]\n",
        "\"\"\"\n",
        "\n",
        "!sudo apt update\n",
        "!sudo apt install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_serve():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(5)"
      ],
      "metadata": {
        "id": "ErhqqJy077Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ollama model pulling\n",
        "!ollama pull granite4:350m\n",
        "# [qwen3:0.6b, granite4:350m, smollm2:135m, smollm2:360m]\n",
        "\"\"\"\n",
        "ResponseError: model requires more system memory\n",
        "(13.0 GiB) than is available (11.4 GiB) (status code: 500) -> granite4:1b\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vEThE8u3-G3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VT-Ntoue7p_K"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.ollama import Ollama\n",
        "\n",
        "llm = Ollama(model=\"granite4:350m\", request_timeout=120.0)\n",
        "\n",
        "response = llm.complete(\"Is it true that angels built the Angeloktisti church in one night?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kFrK9LXpGahf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}